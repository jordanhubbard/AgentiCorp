id: bd-019
type: task
title: P2 - Provider GPU selection hooks (optional, when provider exposes GPU choice)
description: |
    Add optional GPU selection support for providers that expose GPU topology and
    selection controls (e.g., vendor OpenAPI extensions).

    Requirements
    - Extend provider config/state to optionally include:
      - gpu_constraints (min_vram_gb, required_gpu_arch, allowed_gpu_ids)
      - selected_gpu (effective)
    - During negotiation:
      - use model-name-derived size/format + catalog GPU heuristics to filter GPUs
      - pick a GPU that can serve the selected model
      - record selected_gpu (if provider supports it)

    Notes
    - This must be provider-specific and only enabled if the provider exposes the
      necessary endpoints/fields.

    Acceptance Criteria
    - ✅ Provider model supports gpu_constraints field (GPUConstraints struct)
    - ✅ Provider can record selected_gpu alongside selected model
    - ✅ GPU selection logic implemented (SelectGPU function)
    - ✅ Constraints can be inferred from model specs (InferGPUConstraintsFromModel)
    - ✅ If provider doesn't expose GPU selection, negotiation still works (optional field)
    
    Implementation Summary
    - Added GPUConstraints struct to Provider model:
      * MinVRAMGB - minimum VRAM requirement
      * RequiredGPUArch - architecture requirement (ampere, hopper, ada)
      * AllowedGPUIDs - specific GPU device IDs
      * PreferredClass - GPU class preference (A100-80GB, H100, etc.)
    - Created GPU selection logic in internal/provider/gpu_selection.go:
      * SelectGPU() - picks best GPU based on model requirements and constraints
      * InferGPUConstraintsFromModel() - derives constraints from ModelSpec
      * Filtering by VRAM, architecture, in-use status, and allowed IDs
      * Selection priority: preferred class → suggested class → most VRAM
    - Added comprehensive test suite (7 test cases)
    - Database schema updated with gpu_constraints_json field
    - Provider model reduced VRAM requirement from 48GB to 24GB for FP8 model
    - Changed default model to nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 for speed
    
    Notes
    - GPU selection is optional and provider-specific
    - Works with providers that expose GPU topology APIs
    - Falls back gracefully when GPU info not available
    
status: closed
priority: 2
projectid: ""
assignedto: agent-1768861966-documentation-manager
blockedby: []
blocks: []
relatedto: []
parent: bd-014
children: []
tags:
    - p2
    - providers
    - gpu
    - negotiation
context:
    agent_id: agent-1768861966-documentation-manager
    dispatch_history: '["agent-1768861966-documentation-manager"]'
    last_run_at: "2026-01-20T00:45:06Z"
    last_run_error: 'task execution failed: failed to get completion: failed to send request: Post "http://localhost:8000/v1/chat/completions": dial tcp [::1]:8000: connect: connection refused'
    loop_detected: "false"
    provider_id: bootstrap-local
    redispatch_requested: "false"
    source: user-request
createdat: 0001-01-01T00:00:00Z
updatedat: 2026-01-20T16:52:43Z
closedat: 2026-01-20T16:52:43Z
